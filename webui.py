# -*- coding: utf-8 -*-
"""
webui.py
"""
import argparse
import os

import gradio as gr
from loguru import logger

from chatpdf import ChatPDF

pwd_path = os.path.abspath(os.path.dirname(__file__))

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument("--gen_model_type", type=str, default="auto")
    parser.add_argument("--gen_model_name", type=str, default="01-ai/Yi-6B-Chat")
    parser.add_argument("--lora_model", type=str, default=None)
    parser.add_argument("--rerank_model_name", type=str, default="maidalun1020/bce-reranker-base_v1")
    parser.add_argument("--device", type=str, default=None)
    parser.add_argument("--corpus_files", type=str, default="sample.pdf")
    parser.add_argument("--int4", action='store_true', help="use int4 quantization")
    parser.add_argument("--int8", action='store_true', help="use int8 quantization")
    parser.add_argument("--chunk_size", type=int, default=220)
    parser.add_argument("--chunk_overlap", type=int, default=0)
    parser.add_argument("--num_expand_context_chunk", type=int, default=1)
    parser.add_argument("--server_name", type=str, default="0.0.0.0")
    parser.add_argument("--server_port", type=int, default=8082)
    parser.add_argument("--share", action='store_true', help="share model")
    args = parser.parse_args()
    logger.info(args)

    model = ChatPDF(
        generate_model_type=args.gen_model_type,
        generate_model_name_or_path=args.gen_model_name,
        lora_model_name_or_path=args.lora_model,
        corpus_files=args.corpus_files.split(','),
        device=args.device,
        int4=args.int4,
        int8=args.int8,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        num_expand_context_chunk=args.num_expand_context_chunk,
        rerank_model_name_or_path=args.rerank_model_name,
    )
    logger.info(f"chatpdf model: {model}")

    # èŠå¤©æœºå™¨äººçš„æµå¼é¢„æµ‹æ¥å£,è°ƒç”¨ChatPDFçš„predict_streamå‡½æ•°ç”Ÿæˆå›ç­”
    def predict_stream(message, history):
        history_format = []
        # **æœ‰ç‚¹é—®é¢˜ï¼Œhistoryå¯¹äºæµå¼æ¥è¯´å…¶å®æ˜¯ç©ºçš„** #
        for human, assistant in history:
            history_format.append([human, assistant])
        model.history = history_format
        # **æœ‰ç‚¹é—®é¢˜** #
        for chunk in model.predict_stream(message):
            yield chunk

    # èŠå¤©æœºå™¨äººçš„éæµå¼é¢„æµ‹æ¥å£,è°ƒç”¨ChatPDFçš„predictå‡½æ•°ç”Ÿæˆå›ç­”
    def predict(message, history):
        logger.debug(message)
        response, reference_results = model.predict(message)
        r = response + "\n\n" + '\n'.join(reference_results)
        logger.debug(r)
        return r

    # Gradioçš„Chatbotç»„ä»¶,ç”¨äºæ˜¾ç¤ºèŠå¤©è®°å½•
    chatbot_stream = gr.Chatbot(
        height=600,
        avatar_images=(
            os.path.join(pwd_path, "assets/user.png"),
            os.path.join(pwd_path, "assets/llama.png"),
        ), bubble_full_width=False)

    title = " ğŸ‰ChatPDF WebUIğŸ‰ "
    description = "Link in Github: [ShuaiLv-JNU/gzis-rag](https://github.com/ShuaiLv-JNU/gzis-rag)"
    css = """.toast-wrap { display: none !important } """
    examples = ['Can you tell me about the NLP?', 'ä»‹ç»ä¸‹NLP']
    # Gradioçš„ChatInterfaceç»„ä»¶,å®ƒå°†å‰é¢å®šä¹‰çš„å„ç§å…ƒç´ ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„èŠå¤©ç•Œé¢
    chat_interface_stream = gr.ChatInterface(
        predict_stream,
        textbox=gr.Textbox(lines=4, placeholder="Ask me question", scale=7),
        title=title,
        description=description,
        chatbot=chatbot_stream,
        css=css,
        examples=examples,
        theme='soft',
    )
    # Gradioçš„Blockså¸ƒå±€ä¸­,å¹¶è°ƒç”¨launch()æ–¹æ³•æ¥å¯åŠ¨WebæœåŠ¡
    with gr.Blocks() as demo:
        chat_interface_stream.render()
    demo.queue().launch(
        server_name=args.server_name, server_port=args.server_port, share=args.share
    )
